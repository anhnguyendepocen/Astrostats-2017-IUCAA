{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "Bayes theorem for model $M_1$ is\n",
    "\\begin{equation}\n",
    "P(M_1 | D) = \\frac{P(D | M_1) P(M_1)}{P(D)}\n",
    "\\end{equation}\n",
    "and likewise for model $M_2$. By taking the ratio, $P(D)$ cancels to leave the \n",
    " {\\em posterior odds ratio}\n",
    "\\begin{equation}\n",
    "\\frac{P(M_1 | D)}{P(M_2 | D)} \\,=\\, \\frac{P(D  |  M_1) P(M_1)}{P(D  |  M_2) P(M_2)} \\ .\n",
    "\\end{equation}\n",
    "The term $P(D  |  M)$ is called the {\\em evidence} for model $M$. It plays a key role in model comparison.\n",
    "If we can't decide a priori between the two models, then we set $P(M_1) = P(M_2)$. These cancel to leave the ratio of the evidences, which is called the {\\em Bayes factor}\n",
    "\\begin{equation}\n",
    "BF_{12} \\,=\\, \\frac{P(D  |  M_1)}{P(D  |  M_2)}\n",
    "\\end{equation}\n",
    "of model 1 to 2. Note that $BF_{21} = BF_{12}^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model $M$ may contain one or more parameters $\\theta$.\n",
    "We have seen how, using the likelihood and prior, we can infer the posterior $P(\\theta  |  D, M)$ for these parameters using Bayes equation,\n",
    "\\begin{equation}\n",
    "P(\\theta  |  D, M) \\,=\\, \\frac{P(D  |  \\theta, M)P(\\theta  |  M)}{P(D  |  M)} \\ .\n",
    "\\end{equation}\n",
    "The denominator, which we previously treated as just a normalization constant for the posterior, is the evidence. It follows from the rules of probability that\n",
    "\\begin{equation}\n",
    "P(D  |  M) \\,=\\, \\int \\, \\underbrace{P(D  |  \\theta, M)}_\\text{likelihood} \\, \\underbrace{P(\\theta  |  M)}_\\text{prior} \\, d\\theta \\ .\n",
    "\\end{equation}\n",
    "The evidence is the integral of the likelihood over the prior, and for this reason is often called the {\\em marginal likelihood}. It tells us how probable the data are under the model, independent of specific values of $\\theta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application\n",
    "\n",
    "In only a few cases can this integral be computed analytically. In general we must compute it numerically. Here is an example.\n",
    "\n",
    "Given a two-dimensional set of points $\\{x_i,y_i\\}$ with noise only on the $y$ values was ask whether there a significant probability for a non-zero gradient of a fitted line. One way to solve this problem is to ask ''what is the Bayes factor $B_{12} = P(D | M_1)/P(D | M_2)$, where $M_1$ is a model with a zero gradient and $M_2$ is a model with an unknown gradient?''. \n",
    "\n",
    "Let us assume that the generative model is $f(x)$ and that the likelihood is Gaussian (and independent for each data point). The evidence is then\n",
    "\\begin{equation}\n",
    "P(D  |  M) \\,=\\, \\int \\, \\prod_i \\frac{1}{\\sigma\\sqrt{2 \\pi}} \\exp{ \\left[ -\\frac{[y_i - f(x_i)]^2}{2\\sigma^2} \\right]} \\, P(\\theta | M) \\, d\\theta \n",
    "\\end{equation}\n",
    "where $\\theta$ includes all the parameters in $f(x)$ as well as $\\sigma$ (which I assume to be the same for all data points).\n",
    "For the models and priors we consider, this integral cannot be performed analytically, so we use a Monte Carlo approximation. The above integral (marginal likelihood) is then estimated as\n",
    "\\begin{equation}\n",
    "P(D  |  M) \\,\\simeq\\, \\frac{1}{N_s}\\sum_{l=1}^{l=N_s} P(D  |  \\theta_l, M)\n",
    "\\end{equation}\n",
    "where the samples $\\{\\theta_l\\}$ have been drawn from the prior. That is, the evidence is the average of the likelihood over a set of samples drawn from the prior. Normally priors are easy to drawn from, so we don't usually need MCMC to do this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "The above is investigated by the code linearmodel_evidence.R in the zip file. It should be self-explanatory (and it is much simpler than the code for sampling from the posterior, because it doesn't use MCMC, and does less plotting).\n",
    "\n",
    "The code computes the evidence for two models:\n",
    "* $M_2$ is a general straight line, $y = b_0 + x\\tan\\alpha + \\epsilon$, where $b_0$ and $\\alpha$ are unknown parameters, and $\\epsilon \\sim {\\cal N}(0, \\sigma)$, which is the Gaussian random noise used to define the likelihood.\n",
    "* $M_1$ is the same as $M_2$, but with zero gradient, $\\tan\\alpha=0$.\n",
    "\n",
    "For the purpose of this demonstration I generate data at random. Ten $x$ values are drawn from ${\\cal U}(-10,10)$, and the $y$ value at each is computed from a straight line model with $b_0=0$ and $\\tan\\alpha =0.1$, to which zero mean Gaussian noise with $\\sigma=1$ is added. $M_2$ is in principle a better description than $M_1$ for these data, but which model is favoured depends on the actual data; the noise is large so the data could, just by chance, favour $M_1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Run the code, look at the resulting plots, and think about what it means. Making the following modifications (one at a time), should help you understand how the Bayesian evidence works.\n",
    "* Change the number of data points.\n",
    "* Change the gradient of the line the data are drawn from.\n",
    "* Change the parameters of the priors (one at a time).\n",
    "* Draw the data from a line with zero gradient rather than a line with finite gradient. With enough data you should find that the simpler model (M1) is favoured. Why is this, when the more complex model (M2) is more flexible? (Once you understand this, you understand the main concept of Bayesian model comparison!)\n",
    "* Draw the data not from a straight line, but from a model which is mildly quadratic. Neither M1 nor M2 is now the \"true\" model, but does it makes sense, which one fits best? (In the real world we never know the \"true\" model; we just fit the best model we can do the data we have.)\n",
    "* Fit different models, e.g. quadratic, sinusoid, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
